{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/izapreev/Projects/SimpDOM'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source code auto reloads into the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help preventing shared maemory errors\n",
    "!ulimit -n 500000\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from Utils.logger import logger\n",
    "from train_and_eval import load_dict\n",
    "from Model.SimpDOM_model import SeqModel\n",
    "from Prediction.test_step import main as get_predictions\n",
    "from Utils.pretrainedGloVe import pretrainedWordEmeddings\n",
    "from DataLoader.swde_dataLoader import swde_data_test, collate_fn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configurations\n",
    "\n",
    "datapath = './data'\n",
    "random.seed(7)\n",
    "device = 'cpu'\n",
    "\n",
    "n_workers=0 # Important to keep this at zero as otherwise we get a shared memory error\n",
    "n_gpus=0\n",
    "char_emb_dim = 16\n",
    "char_hid_dim = 100\n",
    "char_emb_dropout = 0.1\n",
    "\n",
    "tag_emb_dim = 16\n",
    "tag_hid_dim = 30\n",
    "\n",
    "leaf_emb_dim = 30\n",
    "pos_emb_dim = 20\n",
    "word_emb_filename= f'{datapath}/glove.6B.100d.txt'\n",
    "\n",
    "train_websites = ['auto-aol','auto-yahoo','auto-motortrend','auto-autobytel', 'auto-msn', ]\n",
    "val_websites = ['auto-aol','auto-yahoo']\n",
    "attributes = ['model', 'price', 'engine', 'fuel_economy']\n",
    "n_classes = len(attributes)+1\n",
    "class_weights = [1,100,100,100,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:28:35 INFO (train_and_eval:36): Loading ./data/English_charDict.pkl\n",
      "11:28:35 INFO (train_and_eval:38): Dictionary ./data/English_charDict.pkl length: 85\n",
      "11:28:35 INFO (train_and_eval:36): Loading ./data/HTMLTagDict.pkl\n",
      "11:28:35 INFO (train_and_eval:38): Dictionary ./data/HTMLTagDict.pkl length: 39\n",
      "11:28:35 INFO (pretrainedGloVe:9): Loading pretrained word emeddings from: ./data/glove.6B.100d.txt\n",
      "11:28:39 INFO (pretrainedGloVe:22): Loaded 400000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained embeddings\n",
    "charDict = load_dict(f'{datapath}/English_charDict.pkl')\n",
    "tagDict = load_dict(f'{datapath}/HTMLTagDict.pkl')\n",
    "WordEmeddings = pretrainedWordEmeddings(word_emb_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:28:39 INFO (981812523:1): Start generating test dataset\n",
      "11:28:39 INFO (swde_dataLoader:261): Start loading data set for websites: ['auto-aol', 'auto-yahoo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86937eb28cf244ba929f21b9282b19da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web site:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264bdbbed20948768e0bb419bdc3dbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web pages:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308c12193987494bb2ce18b9f7bd2adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web pages:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:28:54 INFO (981812523:5): Finished creating the test dataset!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 397233 - nodes are loaded in swde_dataLoader_test\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Start generating test dataset')\n",
    "test_dataset = DataLoader(dataset = swde_data_test(val_websites, datapath, charDict, \\\n",
    "                                  tagDict, n_gpus, WordEmeddings), num_workers=n_workers, \\\n",
    "                                  batch_size=32, shuffle=False, pin_memory = True, collate_fn = collate_fn_test)\n",
    "logger.info(f'Finished creating the test dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:28:54 INFO (2635199416:1): Instantiating the Model checkpoint\n",
      "11:28:54 INFO (2635199416:31): Model config: {'out_dim': 5, 'train_websites': ['auto-aol', 'auto-yahoo', 'auto-motortrend', 'auto-autobytel', 'auto-msn'], 'val_websites': ['auto-aol', 'auto-yahoo'], 'datapath': './data', 'n_workers': 0, 'charDict': {'o': 1, 'g': 2, 'n': 3, 'e': 4, '\\n': 5, 'c': 6, 'k': 7, '0': 8, '1': 9, 'b': 10, 'm': 11, '-': 12, '2': 13, 'y': 14, 'l': 15, ' ': 16, 'u': 17, 'h': 18, 'a': 19, 's': 20, 'r': 21, 'w': 22, 'p': 23, 't': 24, 'i': 25, 'f': 26, 'd': 27, '&': 28, 'v': 29, 'q': 30, '.': 31, '4': 32, ':': 33, '3': 34, '°': 35, '6': 36, 'x': 37, '$': 38, '5': 39, 'j': 40, 'z': 41, '9': 42, ',': 43, '8': 44, '?': 45, '7': 46, \"'\": 47, ')': 48, '(': 49, '\\t': 50, '@': 51, '/': 52, '%': 53, '*': 54, '=': 55, '®': 56, '©': 57, '–': 58, '\"': 59, ';': 60, '!': 61, '+': 62, '{': 63, '|': 64, '»': 65, '’': 66, '>': 67, '#': 68, '<': 69, '—': 70, '[': 71, ']': 72, 'ü': 73, '“': 74, '”': 75, '≈': 76, '_': 77, '\\xa0': 78, '…': 79, '•': 80, 'ã': 81, '¼': 82, 'ú': 83, 'ñ': 84, '\\x96': 85}, 'char_emb_dim': 16, 'char_hid_dim': 100, 'char_emb_dropout': 0.1, 'tagDict': {'html': 1, 'title': 2, 'head': 3, 'a': 4, 'body': 5, 'div': 6, 'li': 7, 'ul': 8, 'span': 9, 'strong': 10, 'img': 11, 'h': 12, 'select': 13, 'form': 14, 'option': 15, 'p': 16, 'br': 17, 'label': 18, 'b': 19, 'th': 20, 'table': 21, 'tr': 22, 'td': 23, 'fieldset': 24, 'nobr': 25, 'sup': 26, 'i': 27, 'input': 28, 'dt': 29, 'dl': 30, 'dd': 31, 'em': 32, 'small': 33, 'cite': 34, 'font': 35, 'hr': 36, 'tbody': 37, 'button': 38, 'center': 39}, 'tag_emb_dim': 16, 'tag_hid_dim': 30, 'leaf_emb_dim': 30, 'pos_emb_dim': 20, 'attributes': ['model', 'price', 'engine', 'fuel_economy'], 'n_gpus': 0, 'class_weights': [1, 100, 100, 100, 100], 'word_emb_filename': './data/glove.6B.100d.txt'}\n"
     ]
    }
   ],
   "source": [
    "logger.info('Instantiating the Model checkpoint')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename='./data/weights',\n",
    "    save_top_k=1,\n",
    "    save_last = True,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'out_dim': n_classes,\n",
    "    'train_websites': train_websites,\n",
    "    'val_websites': val_websites,\n",
    "    'datapath': datapath,\n",
    "    'n_workers': n_workers,\n",
    "    'charDict' : charDict,\n",
    "    'char_emb_dim' : char_emb_dim,\n",
    "    'char_hid_dim' : char_hid_dim,\n",
    "    'char_emb_dropout' : char_emb_dropout,\n",
    "    'tagDict': tagDict,\n",
    "    'tag_emb_dim': tag_emb_dim,\n",
    "    'tag_hid_dim': tag_hid_dim,\n",
    "    'leaf_emb_dim': leaf_emb_dim,\n",
    "    'pos_emb_dim': pos_emb_dim,\n",
    "    'attributes': attributes,\n",
    "    'n_gpus' : n_gpus,\n",
    "    'class_weights':class_weights,\n",
    "    'word_emb_filename': word_emb_filename\n",
    "}\n",
    "logger.info(f'Model config: {config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:25:03 INFO (1667893443:1): Loading the Sequential model from Checkpoint\n",
      "15:25:03 INFO (pretrainedGloVe:9): Loading pretrained word emeddings from: ./data/glove.6B.100d.txt\n",
      "15:25:07 INFO (pretrainedGloVe:22): Loaded 400000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading the Sequential model from Checkpoint')\n",
    "pre_trained_model_weights = f'{datapath}/weights.ckpt'\n",
    "#pre_trained_model_weights = 'weights_wpix_manual_ckpt.ckpt' # This one is to test the re-trianed model\n",
    "model = SeqModel.load_from_checkpoint(pre_trained_model_weights, config=config)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:25:07 INFO (743827731:1): Generating model predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbab33b8fd7b496e9c44f640faa26da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing batches:   0%|          | 0/12414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info('Generating model predictions')\n",
    "df = get_predictions(test_dataset, model, device, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:38:38 INFO (1285270816:2): Dumping predictions dataframe into: test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "dump_file_name = 'test_predictions.csv'\n",
    "logger.info(f'Dumping predictions dataframe into: {dump_file_name}')\n",
    "df.to_csv(dump_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:38:44 INFO (3452662240:3): Prediction summary:\n",
      "{1: (0.5870069605568445, 0.5694618272841051, 0.5781013026927674), 2: (0.6041169451073986, 0.50625, 0.5508705114254625), 3: (0.9382561535252398, 0.1265, 0.22294194408359308), 4: (0.9994998749687422, 0.999479843953186, 0.9994898593606025)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class - 1: precision = 0.5870069605568445, recall = 0.5694618272841051, F1 = 0.5781013026927674\n",
      "class - 2: precision = 0.6041169451073986, recall = 0.50625, F1 = 0.5508705114254625\n",
      "class - 3: precision = 0.9382561535252398, recall = 0.1265, F1 = 0.22294194408359308\n",
      "class - 4: precision = 0.9994998749687422, recall = 0.999479843953186, F1 = 0.9994898593606025\n"
     ]
    }
   ],
   "source": [
    "from Prediction.PRSummary import cal_PR_summary\n",
    "avg_prf1_dict = cal_PR_summary(df, n_classes)\n",
    "logger.info(f'Prediction summary:\\n{avg_prf1_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:52:26 INFO (1702364033:3): Website-level prediction summary:\n",
      "      website  attribute  precision    recall  retrieved_relevant  retrieved  \\\n",
      "0    auto-aol          1   0.359759  0.359398                 717       1993   \n",
      "1    auto-aol          2   1.000000  1.000000                2000       2000   \n",
      "2    auto-aol          3   0.000000  0.000000                   0          4   \n",
      "3    auto-aol          4   0.998916  0.998916                1843       1845   \n",
      "4  auto-yahoo          1   0.826964  0.779000                1558       1884   \n",
      "5  auto-yahoo          2   0.018491  0.012500                  25       1352   \n",
      "6  auto-yahoo          3   0.637280  0.126500                 253        397   \n",
      "7  auto-yahoo          4   1.000000  1.000000                2000       2000   \n",
      "\n",
      "   relevant  \n",
      "0      1995  \n",
      "1      2000  \n",
      "2         0  \n",
      "3      1845  \n",
      "4      2000  \n",
      "5      2000  \n",
      "6      2000  \n",
      "7      2000  \n"
     ]
    }
   ],
   "source": [
    "from Prediction.WebsiteLevel_PR_Generator import cal_PR_summary as websiteLevel_cal_PR_summary\n",
    "pr_summary_df, pr_results_df = websiteLevel_cal_PR_summary(df, n_classes)\n",
    "logger.info(f'Website-level prediction summary:\\n{pr_results_df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
