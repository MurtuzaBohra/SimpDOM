{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -n tfm -c conda-forge pytorch-lightning --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help preventing shared maemory errors\n",
    "!ulimit -n 64000\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from Utils.pretrainedGloVe import pretrainedWordEmeddings\n",
    "from DataLoader.swde_dataLoader import swde_data_test, collate_fn_test\n",
    "from Model.SimpDOM_model import SeqModel\n",
    "from Prediction.test_step import main as get_predictions\n",
    "from Utils.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configurations\n",
    "\n",
    "datapath = './data'\n",
    "random.seed(7)\n",
    "device = 'cpu'\n",
    "\n",
    "n_workers=0 # Must be 0 to prevent shared memory issues!\n",
    "n_gpus=0\n",
    "char_emb_dim = 16\n",
    "char_hid_dim = 100\n",
    "char_emb_dropout = 0.1\n",
    "\n",
    "tag_emb_dim = 16\n",
    "tag_hid_dim = 30\n",
    "\n",
    "leaf_emb_dim = 30\n",
    "pos_emb_dim = 20\n",
    "word_emb_filename= '{}/glove.6B.100d.txt'.format(datapath)\n",
    "\n",
    "train_websites = ['auto-aol','auto-yahoo','auto-motortrend','auto-autobytel', 'auto-msn', ]\n",
    "val_websites = ['auto-aol','auto-yahoo']\n",
    "attributes = ['model', 'price', 'engine', 'fuel_economy']\n",
    "n_classes = len(attributes)+1\n",
    "class_weights = [1,100,100,100,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:40:32 INFO (2422149600:3): Loading ./data/English_charDict.pkl\n",
      "14:40:32 INFO (2422149600:5): Loading ./data/HTMLTagDict.pkl\n",
      "14:40:32 INFO (2422149600:7): Char dictionary length: 85, Tag dictionary length: 39\n",
      "14:40:32 INFO (2422149600:9): Loading pretrained word emeddings from: ./data/glove.6B.100d.txt\n",
      "14:40:36 INFO (2422149600:11): Start generating test dataset\n",
      "14:40:36 INFO (swde_dataLoader:236): Start loading data set for websites: ['auto-aol', 'auto-yahoo']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained word vectors loaded - 400000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e44fb5d56346029d21c3fb9625e696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web site:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f46d8d32ca94bb380eafc6587d37eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web pages:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Loading\n",
    "\n",
    "logger.info(f'Loading {datapath}/English_charDict.pkl')\n",
    "charDict = pickle.load(open('{}/English_charDict.pkl'.format(datapath),'rb'))\n",
    "logger.info(f'Loading {datapath}/HTMLTagDict.pkl')\n",
    "tagDict = pickle.load(open('{}/HTMLTagDict.pkl'.format(datapath),'rb'))\n",
    "logger.info(f'Char dictionary length: {len(charDict)}, Tag dictionary length: {len(tagDict)}')\n",
    "\n",
    "logger.info(f'Loading pretrained word emeddings from: {datapath}/glove.6B.100d.txt')\n",
    "WordEmeddings = pretrainedWordEmeddings('{}/glove.6B.100d.txt'.format(datapath))\n",
    "logger.info(f'Start generating test dataset')\n",
    "test_dataset = DataLoader(dataset = swde_data_test(val_websites, datapath, charDict, \\\n",
    "                                  tagDict, n_gpus, WordEmeddings), num_workers=n_workers, \\\n",
    "                                  batch_size=32, shuffle=False, collate_fn = collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Instantiating the Model checkpoint')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename='./data/weights',\n",
    "    save_top_k=1,\n",
    "    save_last = True,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'out_dim': n_classes,\n",
    "    'train_websites': train_websites,\n",
    "    'val_websites': val_websites,\n",
    "    'datapath': datapath,\n",
    "    'n_workers': n_workers,\n",
    "    'charDict' : charDict,\n",
    "    'char_emb_dim' : char_emb_dim,\n",
    "    'char_hid_dim' : char_hid_dim,\n",
    "    'char_emb_dropout' : char_emb_dropout,\n",
    "    'tagDict': tagDict,\n",
    "    'tag_emb_dim': tag_emb_dim,\n",
    "    'tag_hid_dim': tag_hid_dim,\n",
    "    'leaf_emb_dim': leaf_emb_dim,\n",
    "    'pos_emb_dim': pos_emb_dim,\n",
    "    'attributes': attributes,\n",
    "    'n_gpus' : n_gpus,\n",
    "    'class_weights':class_weights,\n",
    "    'word_emb_filename': word_emb_filename\n",
    "}\n",
    "logger.info(f'Model config: {config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info('Loading the Sequential model from Checkpoint')\n",
    "model = SeqModel.load_from_checkpoint(f'{datapath}/weights.ckpt', config=config)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info('Generating model predictions')\n",
    "df = get_predictions(test_dataset, model, device, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Prediction.PRSummary import cal_PR_summary\n",
    "avg_prf1_dict = cal_PR_summary(df, n_classes)\n",
    "\n",
    "from Prediction.WebsiteLevel_PR_Generator import cal_PR_summary as websiteLevel_cal_PR_summary\n",
    "pr_summary_df, pr_results_df = websiteLevel_cal_PR_summary(df, n_classes)\n",
    "print(pr_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
