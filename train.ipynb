{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help preventing shared maemory errors\n",
    "!ulimit -n 64000\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:33:10 INFO (3548147748:12): Epoch: 1/1, web-sites: ['auto-aol', 'auto-yahoo']\n",
      "13:33:10 INFO (train_and_eval:36): Loading ./data/English_charDict.pkl\n",
      "13:33:10 INFO (train_and_eval:38): Dictionary ./data/English_charDict.pkl length: 85\n",
      "13:33:10 INFO (train_and_eval:36): Loading ./data/HTMLTagDict.pkl\n",
      "13:33:10 INFO (train_and_eval:38): Dictionary ./data/HTMLTagDict.pkl length: 39\n",
      "13:33:10 INFO (train_and_eval:51): Instantiating the Model checkpoint.\n",
      "13:33:10 INFO (train_and_eval:61): Instantiating the Sequential model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training websites - ['auto-aol']\n",
      "validation websites - ['auto-yahoo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:33:15 INFO (train_and_eval:84): Instantiating the Training object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained word vectors loaded - 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izapreev/miniforge3/envs/tfm/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "13:33:15 INFO (train_and_eval:87): Fitting the model\n",
      "\n",
      "  | Name                    | Type             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | charLevelWordEmbeddings | CharCNN          | 6.3 K \n",
      "1 | BiLSTM                  | BiLSTM           | 241 K \n",
      "2 | BiLSTM_xpath            | BiLSTM_xpath     | 12.2 K\n",
      "3 | Leaf_embedding          | Embedding        | 1.2 K \n",
      "4 | pos_embedding           | Embedding        | 2.0 K \n",
      "5 | classifier              | Sequential       | 46.1 K\n",
      "6 | loss                    | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------------------\n",
      "309 K     Trainable params\n",
      "0         Non-trainable params\n",
      "309 K     Total params\n",
      "1.238     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d8f2fa265d408e9e379d8b3c0d64fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web sites:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed1554743c40fdaa3a701823f2306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web pages:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:33:19 INFO (swde_dataLoader:151): SWDE data loader has loaded: 21945 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_val_loss - 1.5399844646453857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izapreev/miniforge3/envs/tfm/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/izapreev/miniforge3/envs/tfm/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f9bc0d129f4a1da390ef48eabf1fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web sites:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9042965bbf4f1685759100782849c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Web pages:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "#n permutations of train and validation sets for average PR results.\n",
    "import random\n",
    "import pickle\n",
    "from train_and_eval import train, test\n",
    "from Utils.logger import logger\n",
    "websites = ['auto-aol','auto-yahoo']#['auto-aol','auto-yahoo','auto-motortrend','auto-autobytel', 'auto-msn' ]\n",
    "attributes = ['model', 'price', 'engine', 'fuel_economy']\n",
    "n=1\n",
    "list_avg_pr = []\n",
    "for itr in range(n):\n",
    "    random.shuffle(websites)\n",
    "    logger.info(f'Epoch: {itr+1}/{n}, web-sites: {websites}')\n",
    "    val_websites, charDict, tagDict, model, n_classes = train(websites, attributes)\n",
    "    logger.info(f'Training is finished, starting predicting')\n",
    "    avg_pr_dict = test(val_websites, charDict, tagDict, model, n_classes)\n",
    "    logger.info(f'Test predictions result: {avg_pr_dict}')\n",
    "    list_avg_pr.append(avg_pr_dict)\n",
    "    pickle.dump(list_avg_pr, open('./list_avg_pr.pkl', 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Create the training data for other verticals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy\n",
    "import os\n",
    "from Utils.DOMTree import DOMTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datapath = './data' #directory path where each vertical's html pages from SWDE dataset (website wise folders)\n",
    "vertical = 'auto'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset processor: to find the set of fixed nodes for each of the websites\n",
    "from DatasetCreation.storeFixedNodes import main\n",
    "main(Datapath, vertical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset processor: Store each website as pickle file\n",
    "from DatasetCreation.storeAllTextNodes import main\n",
    "main(Datapath, vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset processor: append the friendCircle of each node in the DOMTree\n",
    "from Utils.friendCircleExtractioin import main\n",
    "main(Datapath, vertical, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset processor: associate the ground truth label to each node\n",
    "attributes = ['model', 'manufacturer', 'price']\n",
    "from DatasetCreation.assignGroundTruthToEachNode import main\n",
    "main(Datapath, vertical, attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
